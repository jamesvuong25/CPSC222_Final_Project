{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   height(cm)  weight(kg) t-shirt size\n",
      "0         158          58            M\n",
      "1         163          61            M\n",
      "2         165          61            L\n",
      "3         168          66            L\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"shirt_sizes.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to ML (Machine Learning)\n",
    "* Supervised learning: labeled data (e.g. there is an attribute (AKA feature) that you are interested in predicting for unseen instances)\n",
    "    * The attribute is called the \"class\" or the \"class label\"\n",
    "    * The attribute is categorical... classification\n",
    "    * The attribute is numeric... regression\n",
    "    * Example algorithm: kNN (k nearest neighbors)\n",
    "* Unsupervised learning: unlabeled data \n",
    "    * Example algorithm: k-means clustering\n",
    "    \n",
    "## Supervised Learning\n",
    "* Need a way to divide a dataset into a training set and a testing set\n",
    "    * The training set is used to build/train a algorithm/model\n",
    "    * The testing set is used to evaluate the algorithm/model\n",
    "    * The training set and the testing set *are different*\n",
    "* Example\n",
    "    * We have this super tiny t-shirt sizes dataset\n",
    "        * 4 instances\n",
    "        * 3 attributes (1 is the class (t-shirt size))\n",
    "        * Goal is to use height and weight attributes to predict t-shirt size\n",
    "        * We will do this for a test set with a single unseen instance\n",
    "            * height=161 weight=63 t-shirt size=?\n",
    "            * Let's say the \"ground truth value\" is M (medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN Algorithm\n",
    "* Identify the k nearest neighbors in the training set to a test set instance\n",
    "    * The most frequently occuring class label amongst the k nearest neighbors will be the class label prediction for the unseen instance\n",
    "* We need a way to measure \"nearness\" AKA \"closeness\"\n",
    "    * 2D: Pythagorean theorem\n",
    "    * ND: Euclidean distance formula = $dist(a, b) = \\sqrt{\\sum_{i=1}^{n} (a_i - b_i)^2}$\n",
    "* We need to normalize (AKA scale) our attributes so we don't have an unanticipated weighting of one attribute more than another (e.g. height has a larger scale then weight, so it will dominate the formula)\n",
    "    * We will use the min-max scaling approach\n",
    "    * For each attribute, the min becomes 0 and the max becomes 1 (e.g. bounded to [0, 1] so the units have no weighting effect)\n",
    "    * For each attribute, for each value subtract the min then divide by the original range (max - min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   height(cm)  weight(kg)\n",
      "0         158          58\n",
      "1         163          61\n",
      "2         165          61\n",
      "3         168          66\n",
      "0    M\n",
      "1    M\n",
      "2    L\n",
      "3    L\n",
      "Name: t-shirt size, dtype: object\n",
      "[[161, 63]]\n"
     ]
    }
   ],
   "source": [
    "# kNN algorithm with the sci-kit learn library\n",
    "# notation\n",
    "# X: is a feature matrix (rows of feature vectors (instances)) with the class labels stripped off\n",
    "# y: is a class label vector\n",
    "# X and y are parallel\n",
    "# use _train and _test to denote train and test sets respectively\n",
    "X_train = df.drop(\"t-shirt size\", axis=1) # 1 is for columns\n",
    "print(X_train)\n",
    "y_train = df[\"t-shirt size\"]\n",
    "print(y_train)\n",
    "X_test = [[161, 63]]\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.   ]\n",
      " [0.5   0.375]\n",
      " [0.7   0.375]\n",
      " [1.    1.   ]]\n",
      "[[0.3   0.625]]\n"
     ]
    }
   ],
   "source": [
    "# normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_normalized = scaler.transform(X_train) # often combined fit_transform()\n",
    "print(X_train_normalized)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "print(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up knn classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=3, metric=\"euclidean\")\n",
    "knn_clf.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y predicted: ['M']\n",
      "nearest neighbors: (array([[0.32015621, 0.47169906, 0.69327123]]), array([[1, 2, 0]]))\n"
     ]
    }
   ],
   "source": [
    "# to make predictions with the classifier for unseen instance\n",
    "y_predicted = knn_clf.predict(X_test_normalized)\n",
    "print(\"y predicted:\", y_predicted)\n",
    "print(\"nearest neighbors:\", knn_clf.kneighbors(X_test_normalized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Thoughts on kNN\n",
    "* What if our attributes are not numeric (meaning they are categorical?)\n",
    "    * Simple approach: convert category labels to integers\n",
    "        * `from sklearn.preprocessing import LabelEncoder`)\n",
    "    * Another approach: write your own distance function (0 if the values are the same, 1 otherwise)\n",
    "        * $dist(v_1, v_2) = 0$ if $v_1 == v_2$, 1 otherwise\n",
    "* kNN is not the only ML algorithm!!!\n",
    "    * Naive Bayes\n",
    "    * Decision trees (random forests)\n",
    "    * SVMs (support vector machines)\n",
    "    * Neural networks\n",
    "    * etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.        ]\n",
      " [0.         0.1       ]\n",
      " [0.         0.5       ]\n",
      " [0.16666667 0.1       ]\n",
      " [0.16666667 0.2       ]\n",
      " [0.41666667 0.2       ]\n",
      " [0.41666667 0.3       ]\n",
      " [0.16666667 0.6       ]\n",
      " [0.41666667 0.6       ]\n",
      " [0.58333333 0.3       ]\n",
      " [0.58333333 0.4       ]\n",
      " [0.58333333 0.7       ]\n",
      " [0.83333333 0.4       ]\n",
      " [0.83333333 0.5       ]\n",
      " [0.83333333 0.8       ]\n",
      " [1.         0.5       ]\n",
      " [1.         0.6       ]\n",
      " [1.         1.        ]]\n",
      "0     M\n",
      "1     M\n",
      "2     M\n",
      "3     M\n",
      "4     M\n",
      "5     M\n",
      "6     M\n",
      "7     L\n",
      "8     L\n",
      "9     L\n",
      "10    L\n",
      "11    L\n",
      "12    L\n",
      "13    L\n",
      "14    L\n",
      "15    L\n",
      "16    L\n",
      "17    L\n",
      "Name: t-shirt size, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# warm up task\n",
    "long_df = pd.read_csv(\"shirt_sizes_long.csv\")\n",
    "\n",
    "X = long_df.drop(\"t-shirt size\", axis=1) # 1 is for columns\n",
    "y = long_df[\"t-shirt size\"]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Evaluation\n",
    "* In our previous demo, we had 1 instance in our \"test set\"\n",
    "    * If our classifier predicted this instance's class correctly, accuracy 100%\n",
    "    * If our classifier predicted this instance's class incorrectly, accuracy 0%\n",
    "* Notes\n",
    "    * We should use a \"large\" test set to get a better picture of how our classifier is performing\n",
    "    * Accuracy doesn't tell the whole story... \n",
    "        * E.g. 100 samples... 99 M, 1 L \n",
    "        * And our classifier simply only predicts M\n",
    "        * We have 99% accuracy, woohoo!!!\n",
    "        * Accuracy only makes sense when your class labels are near evenly distributed\n",
    "* Given a dataset, we need a way to \"divide\" our dataset into a training set and a test set\n",
    "    * A few ways to do this..\n",
    "        1. Hold out method\n",
    "        1. Random subsampling\n",
    "        1. Cross validation\n",
    "        1. Bootstrap method\n",
    "\n",
    "## Hold out Method\n",
    "* \"hold out\" a certain number or percentage of instances in a dataset for testing\n",
    "    * Train on the remaining instances\n",
    "    * Typically choose a standard split or percentage\n",
    "        * E.g. 2:1 split: 1/3 of data held out for testing; 2/3 used for training\n",
    "        * E.g. 25% of data held out for testing; 75% used for training\n",
    "            * Default for sklearn's `train_test_split()`\n",
    "            * https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5\n",
      "['L' 'M' 'M' 'M' 'L']\n",
      "['L', 'M', 'L', 'M', 'L']\n",
      "0.8\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(len(X) * 0.25)\n",
    "# random_state is used for reproducibility\n",
    "# stratify to ensure a similar distribution of classs labels\n",
    "# in your training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, stratify=y)\n",
    "# print(X_train)\n",
    "# print(y_train)\n",
    "# print(X_test)\n",
    "# print(y_test)\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=3, metric=\"euclidean\")\n",
    "knn_clf.fit(X_train, y_train)\n",
    "y_predicted = knn_clf.predict(X_test)\n",
    "print(y_predicted)\n",
    "print(list(y_test)) # 80%\n",
    "accuracy = knn_clf.score(X_test, y_test)\n",
    "print(accuracy)\n",
    "accuracy = accuracy_score(y_test, y_predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L' 'M' 'L' 'M' 'L']\n",
      "['L', 'M', 'L', 'M', 'L']\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# do again with a decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=0)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_predicted = tree_clf.predict(X_test)\n",
    "print(y_predicted)\n",
    "print(list(y_test)) # 100%\n",
    "accuracy = tree_clf.score(X_test, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Subsampling\n",
    "* Perform the hold out method k times (diff k from kNN)\n",
    "* Accuracy is the mean accuracy over the k runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "* With random subsampling, we are not guaranteed that each instance ends up in a test set at least once\n",
    "* With cross validation, we are more intentional about our \"partitions\"\n",
    "* Algorithm: Divide the dataset into k folds (also a diff k)\n",
    "    * For each fold:\n",
    "        * Hold out the fold and test on it\n",
    "        * Train on the remaining folds (folds - fold)\n",
    "* With this algorithm, each instance is tested exactly 1 time\n",
    "* Accuracy is the total predicted correctly divided by the total predicted\n",
    "    * https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.neighbors.classification.KNeighborsClassifier'>\n",
      "[0.6        1.         1.         1.         0.66666667] 0.8533333333333333\n",
      "['M' 'M' 'M' 'M' 'M' 'M' 'L' 'M' 'L' 'M' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L']\n",
      "0.8333333333333334\n",
      "<class 'sklearn.tree.tree.DecisionTreeClassifier'>\n",
      "[0.6        0.75       1.         1.         0.66666667] 0.8033333333333333\n",
      "['M' 'M' 'L' 'M' 'M' 'M' 'L' 'M' 'M' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L']\n",
      "0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "# run 5-fold cross validation for both the knn and tree\n",
    "for clf in [knn_clf, tree_clf]:\n",
    "    print(type(clf))\n",
    "    # lazy approach for accuracies\n",
    "    accuracies = cross_val_score(clf, X, y, cv=5)\n",
    "    print(accuracies, accuracies.mean())\n",
    "    # better approach for accuracies\n",
    "    y_predicted = cross_val_predict(clf, X, y, cv=5)\n",
    "    print(y_predicted)\n",
    "    accuracy = accuracy_score(y, y_predicted)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GS for next class...\n",
    "\n",
    "NOTE: from [cross_val_score() documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html): \"For int/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used. These splitters are instantiated with shuffle=False so the splits will be the same across calls.\"\n",
    "\n",
    "TODO: cross validation variants plus confusion matrices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
